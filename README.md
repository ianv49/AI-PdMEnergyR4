# AI-EnergyForcastR4
AI-Driven Predictive Maintenance for Renewable Energy Assets 
# AI-Driven Predictive Maintenance for Renewable Energy Assets

This project develops a cross-platform application for predictive maintenance of renewable energy assets (wind turbines, solar panels, inverters, batteries). It uses IoT sensor data, external weather/solar APIs, and AI/ML models to forecast failures and optimize maintenance schedules.

---

## ðŸš€ Features
- Real-time sensor data ingestion (temperature, humidity, irradiance, wind speed).
- External API integration (OpenWeather, NASA POWER, Tomorrow.io).
- Local PostgreSQL + TimescaleDB storage for time-series data.
- Preprocessing scripts for normalization, cleaning, and interpolation.
- Ready for deployment on Raspberry Pi 4, but fully compatible with Mac and Windows laptops during development.

---

## ðŸ› ï¸ Development Setup

### 1. Clone Repository
```bash
AI-EnergyForcastR4/
â”‚
â”œâ”€â”€ db/                     # Database setup and connectors
â”‚   â”œâ”€â”€ db_connector.py     # Python script to handles DB connection
â”‚   â”œâ”€â”€ db_ingest.py        # Python script to ingestion script (imports connector)
â”‚   â”œâ”€â”€ test_connection.py  # Python script for quick connection test
â”‚   â””â”€â”€ schema.sql          # SQL table definitions
â”‚
â”œâ”€â”€ data/                 
â”‚   â”œâ”€â”€ sensor_logs.txt   # plain text log file 
â”‚   â””â”€â”€ sensor_data.csv   # CSV file 
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ requirements.txt      # List of Python dependencies
â”œâ”€â”€ README.md             # Documentation for setup and usage
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ ingestion.log         # todayâ€™s log)
â”‚   â””â”€â”€ ingestion.log.2026-01-20  #yesterdayâ€™s log, autoâ€‘created at midnight
â”‚
â”œâ”€â”€ sensors/              # Scripts for sensor data (real or simulated)
â”‚   â””â”€â”€ sensor_ingest.py  # First script: generate or simulate sensor readings
â”‚
â”œâ”€â”€ api_wrappers/         # External API modules
â”‚   â””â”€â”€ openweather.py    # First wrapper: fetch weather data
â”‚   â””â”€â”€ nasa_power.py     # Second wrapper: fetch solar/irradiance data
â”‚
â”‚
â”œâ”€â”€ preprocessing/        # Data cleaning scripts
â”‚   â””â”€â”€ preprocess.py     # First script: normalize and clean sensor logs
â”‚
â””â”€â”€ notebooks/            # Jupyter notebooks for demos
    â””â”€â”€ data_pipeline_demo.ipynb  # Step-by-step interactive demo

#### my notes
'''bash
Perfect ðŸŽ‰ â€” that package list shows your environment is now fully set up and healthy. Youâ€™ve got all the core libraries installed:
requests â†’ API calls and HTTP requests
pandas â†’ data handling and analysis
numpy â†’ numerical computations
psycopg2-binary â†’ PostgreSQL connector
scikit-learn â†’ classical machine learning models
torch â†’ PyTorch deep learning framework
matplotlib â†’ static plotting
plotly â†’ interactive charts
paho-mqtt â†’ IoT sensor communication
Plus all their dependencies (like scipy, joblib, threadpoolctl, etc.). This means youâ€™re ready to start coding your actual project.
PS D:\My Documents\tools\skul\bsu2024\bsu_mot512_thesis1\GithubVisualStudioCode\AI-EnergyForcastR4> python -m pip list
Package            Version
------------------ -----------
certifi            2026.1.4
charset-normalizer 3.4.4
contourpy          1.3.3
cycler             0.12.1
filelock           3.20.3
fonttools          4.61.1
fsspec             2026.1.0
idna               3.11
Jinja2             3.1.6
joblib             1.5.3
kiwisolver         1.4.9
MarkupSafe         3.0.3
matplotlib         3.10.8
mpmath             1.3.0
narwhals           2.15.0
networkx           3.6.1
numpy              2.4.1
packaging          25.0
paho-mqtt          2.1.0
pandas             2.3.3
pillow             12.1.0
pip                25.3
plotly             6.5.2
psycopg2-binary    2.9.11
pyparsing          3.3.1
python-dateutil    2.9.0.post0
pytz               2025.2
requests           2.32.5
scikit-learn       1.8.0
scipy              1.17.0
setuptools         80.9.0
six                1.17.0
sympy              1.14.0
threadpoolctl      3.6.0
torch              2.9.1
typing_extensions  4.15.0
tzdata             2025.3
urllib3            2.6.3

[always used for activating env; "venv\Scripts\activate.bat"]
[This starts PostgreSQL in the background, listening on port 5432.
Since you donâ€™t have admin rights, it wonâ€™t be a Windows service â€” youâ€™ll need to run this manually each time.
in <cmd> <"D:\My Documents\tools\postgresql\pgsql\bin\pg_ctl.exe" -D "D:\My Documents\tools\postgresql\pgsql\data" -l logfile start>]
[Stopping PostgreSQL. When youâ€™re done, stop the server cleanly, This shuts down PostgreSQL safely:
in cmd> <"D:\My Documents\tools\postgresql\pgsql\bin\pg_ctl.exe" -D "D:\My Documents\tools\postgresql\pgsql\data" stop>]
[Restarting PostgreSQL, If you want to restart:
in cmd> <"D:\My Documents\tools\postgresql\pgsql\bin\pg_ctl.exe" -D "D:\My Documents\tools\postgresql\pgsql\data" restart>]

...notes 260119;
Phase,Item,Status
Phase 1: Environment Setup,Install PostgreSQL portable binaries,Done
Phase 1: Environment Setup,Initialize database cluster (initdb),Done
Phase 1: Environment Setup,Start PostgreSQL manually (pg_ctl),Done
Phase 1: Environment Setup,Connect with psql,Done
Phase 2: Database Schema,Create energy_db database,Done
Phase 2: Database Schema,Define sensor_data table schema,Done
Phase 2: Database Schema,Verify schema with \d sensor_data,Done
Phase 3: Python Integration,Install psycopg2 driver,Done
Phase 3: Python Integration,Create db_ingest.py script,Done
Phase 3: Python Integration,Connect Python to PostgreSQL,Done
Phase 3: Python Integration,Insert test row via Python,Done
Phase 3: Python Integration,Fetch and display rows via Python,Done
Phase 4: Log Ingestion,Adapt script to read sensor_logs.txt,Done
Phase 4: Log Ingestion,Insert multiple rows from file,Done
Phase 4: Log Ingestion,Verify ingestion with query output,Done
Phase 5: Enhancements,Handle duplicate entries (unique timestamp + ON CONFLICT),Pending
Phase 5: Enhancements,Format timestamp output (seconds only),Done
Phase 5: Enhancements,Optional: pretty table output,Pending
Phase 6: Next Steps,Automate ingestion (batch file or cron job),Pending
Phase 6: Next Steps,Extend ingestion for CSV/real sensor streams,Pending
Phase 6: Next Steps,Dashboard/visualization integration,Pending
...notes 260120;
sql password = PdM
Phase 1: Environment Setup
Install PostgreSQL portable binaries â†’ Done
Initialize database cluster (initdb) â†’ Done
Start PostgreSQL manually (pg_ctl) â†’ Done
Connect with psql â†’ Done
Phase 2: Database Schema
Create energy_db database â†’ Done
Define sensor_data table schema â†’ Done
Verify schema with \d sensor_data â†’ Done
Phase 3: Python Integration
Install psycopg2 driver â†’ Done
Create db_ingest.py script â†’ Done
Connect Python to PostgreSQL â†’ Done
Insert test row via Python â†’ Done
Fetch and display rows via Python â†’ Done
Phase 4: Log Ingestion
Adapt script to read sensor_logs.txt â†’ Done
Insert multiple rows from file â†’ Done
Verify ingestion with query output â†’ Done
Phase 5 Completion Checklist
Format timestamp output (seconds only) â†’ Done
Pretty table output â†’ Done
Row count before/after ingestion â†’ Done
Skip header line in text ingestion â†’ Done
Modularize connection (db_connector.py) â†’ Done
Add test script (test_connection.py) â†’ Done
Show top/bottom rows in test script â†’ Done
Handle duplicate entries (unique timestamp + ON CONFLICT) â†’ Done 
Phase 6: Next Steps
Automate ingestion (batch file or cron job) â†’ Pending
Extend ingestion for CSV/real sensor streams â†’ Pending
Dashboard/visualization integration â†’ Pending
Add permanent log file output (logs/ingestion.log) â†’ Pending
